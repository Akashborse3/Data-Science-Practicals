{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef6c232",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2285b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Akash\n",
      "[nltk_data]     Borse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenization: \n",
      "['I believe this would help the reader understand how tokenization         works.', 'as well as realize its importance.']\n",
      "Word tokenization:  ['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.', 'as', 'well', 'as', 'realize', 'its', 'importance', '.']\n",
      "Word tokenization with list of list of each word: \n",
      "[['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.'], ['as', 'well', 'as', 'realize', 'its', 'importance', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"I believe this would help the reader understand how tokenization \\\n",
    "        works. as well as realize its importance.\"\n",
    "print(\"Sentence tokenization: \")       \n",
    "sents = (sent_tokenize(text))        \n",
    "print(sents)\n",
    "\n",
    "print (\"Word tokenization: \", word_tokenize(text))\n",
    "\n",
    "\n",
    "print(\"Word tokenization with list of list of each word: \")\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d272b5a",
   "metadata": {},
   "source": [
    "## Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ed5237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Akash\n",
      "[nltk_data]     Borse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62fc318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'believe', 'would', 'help', 'reader', 'understand', 'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "text = \"I believe this would help the reader understand how tokenization \\\n",
    "        works. as well as realize its importance (text) .\"\n",
    "        \n",
    "custom_list = set(stopwords.words('english')+list(punctuation))        \n",
    "        \n",
    "word_list = [word for word in word_tokenize(text) if word not in custom_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c153ab",
   "metadata": {},
   "source": [
    "##  N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16443739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(('I', 'believe'), 1), (('believe', 'would'), 1), (('would', 'help'), 1), (('help', 'reader'), 1), (('reader', 'understand'), 1), (('understand', 'tokenization'), 1), (('tokenization', 'works'), 1), (('works', 'well'), 1), (('well', 'realize'), 1), (('realize', 'importance'), 1), (('importance', 'text'), 1)])\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "word_list = ['I', 'believe', 'would', 'help', 'reader', 'understand', \\\n",
    "             'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n",
    "\n",
    "finde = BigramCollocationFinder.from_words(word_list)\n",
    "print(finde.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9620c",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation(WSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc5eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Akash\n",
      "[nltk_data]     Borse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Akash\n",
      "[nltk_data]     Borse\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf623d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('mouse.n.01') any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
      "Synset('shiner.n.01') a swollen bruise caused by a blow to the eye\n",
      "Synset('mouse.n.03') person who is quiet or timid\n",
      "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
      "Synset('sneak.v.01') to go stealthily or furtively\n",
      "Synset('mouse.v.02') manipulate the mouse of a computer\n",
      "------------WSD---------------\n",
      "For bass: \n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "For mouse: \n",
      "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for ss in wordnet.synsets('mouse'):\n",
    "    print(ss, ss.definition())\n",
    "    \n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"------------WSD---------------\")\n",
    "print(\"For bass: \")\n",
    "context_1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), \"bass\")\n",
    "print(context_1, context_1.definition())\n",
    "\n",
    "context_2 = lesk(word_tokenize(\"The sea bass really very hard to catch\"), \"bass\")\n",
    "print(context_2, context_2.definition())\n",
    "\n",
    "print(\"For mouse: \")\n",
    "context_3 = lesk(word_tokenize(\"My mouse is not working, need to change it\"), \"mouse\")\n",
    "print(context_3, context_3.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2fee65",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d66a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'import', 'to', 'by', 'very', 'python', 'whil', 'you', 'ar', 'python', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "new_text = \"It is important to by very pythonly while you are pythoning\\\n",
    "             with python. All pythoners have pythoned poorly at least once.\"\n",
    "             \n",
    "l_s =  LancasterStemmer()\n",
    "stem_lan =  [l_s.stem(word) for word in word_tokenize(new_text)]\n",
    "print(stem_lan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911406d",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98152ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Text\n",
      "0    This is the first document from heaven\n",
      "1      but the second document is from mars\n",
      "2    And this is the third one from nowhere\n",
      "3  Is this the first document from nowhere?\n",
      "['and', 'but', 'document', 'first', 'from', 'heaven', 'is', 'mars', 'nowhere', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 0 1 1 1 1 1 0 0 0 0 1 0 1]\n",
      " [0 1 1 0 1 0 1 1 0 0 1 1 0 0]\n",
      " [1 0 0 0 1 0 1 0 1 1 0 1 1 1]\n",
      " [0 0 1 1 1 0 1 0 1 0 0 1 0 1]]\n",
      "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n",
      "[[0 0 1 1 1 1 0 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 1 0 0 1 1 0]\n",
      " [1 0 0 0 1 0 0 1 1 0 1 1]\n",
      " [0 0 1 1 1 0 0 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'Text':corpus})\n",
    "print(df)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_v = CountVectorizer()\n",
    "X = count_v.fit_transform(df.Text).toarray()\n",
    "print(count_v.get_feature_names())\n",
    "\n",
    "print(X)\n",
    "print(count_v.vocabulary_)\n",
    "\n",
    "count_v = CountVectorizer(stop_words=['this','is'])\n",
    "X = count_v.fit_transform(df.Text).toarray()\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63935860",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce4745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n",
      "[1.91629073 1.91629073 1.22314355 1.51082562 1.         1.91629073\n",
      " 1.         1.91629073 1.51082562 1.91629073 1.91629073 1.\n",
      " 1.91629073 1.22314355]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e594f",
   "metadata": {},
   "source": [
    "## Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "060e346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0. -1.  1.  0.  0.  0.  0.  0.  2.  0. -1.]\n",
      " [ 0.  0. -1. -1.  0.  0.  0.  0.  0.  0.  0. -1.  2.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "     'This is the first document from heaven',\n",
    "     'but the second document is from mars',\n",
    "     'And this is the third one from nowhere',\n",
    "     'Is this the first document from nowhere?',\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({'Text':corpus})\n",
    "\n",
    "hash_v = HashingVectorizer(n_features=15, norm=None, alternate_sign=True)\n",
    "print(hash_v.fit_transform(df.Text).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d145562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
